{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bb03066-1019-4a4e-aae2-5510690d80cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# Write DynamoDB JSON data in Azure storage (ADLS) to Azure Cosmos DB \n",
    "\n",
    "This notebook demonstrates how to load DynamoDB JSON data from Azure Data Lake Storage Gen2 (ADLS Gen2) and write it to Azure Cosmos DB for NoSQL. The workflow is divided into these steps:\n",
    "\n",
    "**1. Load DynamoDB JSON data from ADLS:** Set up Spark configuration and load DynamoDB compressed JSON data from ADLS Gen2. Optionally, transform DynamoDB data.\n",
    "\n",
    "**2. Load data to Azure Cosmos DB NoSQL using CosmosDB Spark Connector:** Create the Azure Cosmos DB database and container using the Catalog API. Write data to Azure Cosmos DB container. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e39631-f9c9-48ef-8741-c0643bccef72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1: Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9614f6-9456-4591-b997-2a82a557a4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-cosmos azure-mgmt-cosmosdb azure.mgmt.authorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7f02780-4411-44cf-a702-32a507175567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2: Restart the kernel to ensure the updated packages are being used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c8442b-db9f-4df6-a3b9-f6abe6645375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c70497d-9e06-47ae-9ac6-046917916867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3: Connect to the Azure Data Lake Storage Gen2 account, read DynamoDB JSON files \n",
    "Using Apache Spark, and display the loaded data in a Spark DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee7a35c-2cd8-4af9-8b16-18de819b904e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"\"\n",
    "container_name = \"\"\n",
    "# update this as per your AWS S3 bucket\n",
    "file_path = \"AWSDynamoDB/01738047791106-7ba095a9/data/*\"\n",
    "\n",
    "# Construct the ABFS path\n",
    "abfs_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{file_path}\"\n",
    "print(abfs_path)\n",
    "\n",
    "# Microsoft Entra ID application\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "tenant_id = \"\"\n",
    "\n",
    "# Set up Spark configuration for Entra ID authentication\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", \n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Read DynamoDB JSON data from Azure Storage\n",
    "\n",
    "df = spark.read.format(\"json\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(abfs_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98b80a1c-6845-4449-b083-50bf2b6fb47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 4 (optional): This step can be used to further transform the DynamoDB data before writing it to Azure Cosmos DB. For example, it can to be used to generate a unqiue `id` field if not already present in DynamoDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "20c9eb2f-4cc3-4ac5-bb68-6f33a98614e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,monotonically_increasing_id\n",
    "\n",
    "def extract_dynamodb_fields(df):\n",
    "    # Define the list of columns you want to extract dynamically. Replace with the list of columns\\fields you want to extract. \n",
    "    columns_to_select = [\"customer_id\", \"Address\", \"Phone\", \"Email\",\"Name\"] \n",
    "\n",
    "    # DynamoDB possible attribute types\n",
    "    dynamodb_types = [\"S\", \"N\", \"B\", \"BS\", \"BOOL\", \"NS\", \"SS\", \"L\", \"M\", \"NULL\"]\n",
    "\n",
    "    # Generate valid field selections dynamically from \"Item\"\n",
    "    select_expressions = []\n",
    "\n",
    "    # Check if \"Item\" column exists\n",
    "    if \"Item\" in df.columns:\n",
    "        item_schema = df.schema[\"Item\"].dataType  # Get the StructType of the \"Item\" column\n",
    "\n",
    "        for col_name in columns_to_select:\n",
    "            # Check if the attribute exists inside \"Item\"\n",
    "            if col_name in item_schema.fieldNames():  # Correct way to access struct field names\n",
    "                subfields = item_schema[col_name].dataType  # Get subfields of the attribute\n",
    "\n",
    "                for dtype in dynamodb_types:\n",
    "                    if dtype in subfields.fieldNames():  # Check if specific DynamoDB type exists\n",
    "                        field_expr = f\"Item.{col_name}.{dtype}\"\n",
    "                        select_expressions.append(col(field_expr).alias(f\"{col_name}_{dtype}\"))\n",
    "\n",
    "        # Select only the existing fields dynamically\n",
    "        if select_expressions:\n",
    "            transformed_df = df.select(*select_expressions)\n",
    "            transformed_df.show(truncate=False)\n",
    "            return transformed_df\n",
    "        else:\n",
    "            print(\"No valid fields found in 'Item'.\")\n",
    "            return df\n",
    "    else:\n",
    "        print(\"No 'Item' column found in the DataFrame.\")\n",
    "        return df\n",
    "\n",
    "# Call the function to process the provided DataFrame (df)\n",
    "result_df = extract_dynamodb_fields(df)\n",
    "# Adding an 'id' column to the DataFrame as the source doesn't have an 'id' column. Exclude if you already have 'id' column in source. \n",
    "objecttransformed_df_new = result_df.withColumn(\"id\", monotonically_increasing_id().cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46d06a07-6511-4530-846b-ccf09e6284d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 5: Create Azure Cosmos DB database and container using the Catalog API of the Azure Cosmos DB Spark connector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e9c1a7-60d7-47ea-823e-ade52acbaf9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cosmosEndpoint = \"\"\n",
    "resourceGroupName = \"\"\n",
    "subscriptionId = \"\"\n",
    "cosmosDatabaseName = \"demodb\"\n",
    "cosmosContainerName = \"customers\"\n",
    "\n",
    "# Microsoft Entra ID application\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "tenant_id = \"\"\n",
    "\n",
    "partitionKeyPath = \"/id\"\n",
    "throughput = '400'\n",
    "\n",
    "# Configure Catalog API to use Entra ID authentication\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.auth.type\", \"ServicePrincipal\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.account.subscriptionId\", subscriptionId)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.account.resourceGroupName\", resourceGroupName)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.account.tenantId\", tenant_id)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.auth.aad.clientId\", client_id)\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.auth.aad.clientSecret\", client_secret)\n",
    "\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS cosmosCatalog.{};\".format(cosmosDatabaseName))\n",
    "# Provide the manualThroughput value based on the data loading size. \n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS cosmosCatalog.{}.{} using cosmos.oltp TBLPROPERTIES(partitionKeyPath = partitionKeyPath, manualThroughput = throughput)\".format(cosmosDatabaseName, cosmosContainerName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a6ad737-43f3-4d43-aa77-68efcba518cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 6: Write data to Azure Cosmos DB container using the Azure Cosmos DB Spark connector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03e2568-dd86-40a7-bba5-491ee2bc5209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cosmosEndpoint = \"\"\n",
    "cosmosDatabaseName = \"demodb\"\n",
    "cosmosContainerName = \"customers\"\n",
    "subscriptionID=\"\"\n",
    "resourceGroup=\"\"\n",
    "\n",
    "# Microsoft Entra ID application\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "tenant_id = \"\"\n",
    "\n",
    "\n",
    "# Set configuration settings to include Entra ID authentication\n",
    "config = {\n",
    "  \"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
    "  \"spark.cosmos.auth.type\": \"ServicePrincipal\",\n",
    "  \"spark.cosmos.account.tenantId\": tenant_id,\n",
    "  \"spark.cosmos.auth.aad.clientId\": client_id,\n",
    "  \"spark.cosmos.auth.aad.clientSecret\": client_secret,\n",
    "  \"spark.cosmos.database\": cosmosDatabaseName,\n",
    "  \"spark.cosmos.container\": cosmosContainerName,\n",
    "  \"spark.cosmos.account.subscriptionId\": subscriptionID,\n",
    "  \"spark.cosmos.account.resourceGroupName\": resourceGroup\n",
    "}\n",
    "\n",
    "# Write data to Azure Cosmos DB\n",
    "objecttransformed_df_new\\\n",
    "   .write\\\n",
    "   .format(\"cosmos.oltp\")\\\n",
    "   .options(**config)\\\n",
    "   .mode(\"APPEND\")\\\n",
    "   .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DynamoDB to CosmosDB NoSql API Migration_FINAL_WIP",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
